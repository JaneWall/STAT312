[
["index.html", "STAT 312 - Introduction to Data Science Applications Welcome", " STAT 312 - Introduction to Data Science Applications M. Barouti D. Dietz M. Robinson J. Wall Welcome This is the website for “STAT 312 Introduction to Data Science Applications”. What you will learn and how to work the site. This website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License. The book is written in RMarkdown with bookdown. It is automatically rebuilt from source by travis. "],
["introduction.html", "Introduction 0.1 Learning Objectives 0.2 Syllabus and grading 0.3 How to use the course materials", " Introduction What is data science? 0.1 Learning Objectives Objective 1 Objective 2 0.2 Syllabus and grading 0.3 How to use the course materials "],
["visualization-intro.html", "1 Introduction to Data Visualization 1.1 Install R and RStudio (or use RStudio Cloud)", " 1 Introduction to Data Visualization 1.1 Install R and RStudio (or use RStudio Cloud) Basic info on installation and access. More to come. "],
["intro-to-ggplot.html", "2 Intro to ggplot 2.1 Why visualize data? 2.2 Other topics", " 2 Intro to ggplot 2.1 Why visualize data? 2.2 Other topics "],
["text-analysis-intro.html", "3 Introduction to Text Analysis", " 3 Introduction to Text Analysis What this module is about. "],
["text1.html", "4 Text1", " 4 Text1 First chapter in text analysis after the intro. Here is how you link to a YouTube video. "],
["tidy-text-mining-from-stat-413.html", "5 Tidy Text Mining from STAT 413 5.1 Tidy Text Format 5.2 unnest_tokens() on larger text 5.3 Compare word frequencies between authors 5.4 Compare word frequency by author to Austen", " 5 Tidy Text Mining from STAT 413 Learning Objectives Remove stop words and identify frequently used words in a text Compare word counts between different groups of text Resources: Text Mining with R by Julia Silge and David Robinson. 5.1 Tidy Text Format This first video corresponds to the online book sections 1.1 - 1.2 which I suggest you read first. Tidy text format (ttf): A table with one token per row where a token is a meaningful unit of text such as a word, n-gram, sentence or paragraph. Other text mining tools use: strings corpus: contain raw strings annotated with metadata document-term matrix: sparse matrix with each row containing one document and each column one term or word; entries are generally counts or td-idf (term frequency - inverse document freq) text &lt;- c(&quot;If You Forget Me&quot;, &quot;by Pablo Neruda&quot;, &quot;I want you to know&quot;, &quot;one thing.&quot;, &quot;You know how this is:&quot;, &quot;if I look&quot;, &quot;at the crystal moon, at the red branch&quot;, &quot;of the slow autumn at my window,&quot;, &quot;if I touch&quot;, &quot;near the fire&quot;, &quot;the impalpable ash&quot;, &quot;or the wrinkled body of the log,&quot;, &quot;everything carries me to you,&quot;, &quot;as if everything that exists,&quot;, &quot;aromas, light, metals,&quot;, &quot;were little boats&quot;, &quot;that sail&quot;, &quot;toward those isles of yours that wait for me.&quot; ) text #&gt; [1] &quot;If You Forget Me&quot; #&gt; [2] &quot;by Pablo Neruda&quot; #&gt; [3] &quot;I want you to know&quot; #&gt; [4] &quot;one thing.&quot; #&gt; [5] &quot;You know how this is:&quot; #&gt; [6] &quot;if I look&quot; #&gt; [7] &quot;at the crystal moon, at the red branch&quot; #&gt; [8] &quot;of the slow autumn at my window,&quot; #&gt; [9] &quot;if I touch&quot; #&gt; [10] &quot;near the fire&quot; #&gt; [11] &quot;the impalpable ash&quot; #&gt; [12] &quot;or the wrinkled body of the log,&quot; #&gt; [13] &quot;everything carries me to you,&quot; #&gt; [14] &quot;as if everything that exists,&quot; #&gt; [15] &quot;aromas, light, metals,&quot; #&gt; [16] &quot;were little boats&quot; #&gt; [17] &quot;that sail&quot; #&gt; [18] &quot;toward those isles of yours that wait for me.&quot; text_df &lt;- tibble( line = 1:length(text), text = text ) text_df #&gt; # A tibble: 18 x 2 #&gt; line text #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1 If You Forget Me #&gt; 2 2 by Pablo Neruda #&gt; 3 3 I want you to know #&gt; 4 4 one thing. #&gt; 5 5 You know how this is: #&gt; 6 6 if I look #&gt; # ... with 12 more rows text_df %&gt;% unnest_tokens(word, text) #&gt; # A tibble: 80 x 2 #&gt; line word #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1 if #&gt; 2 1 you #&gt; 3 1 forget #&gt; 4 1 me #&gt; 5 2 by #&gt; 6 2 pablo #&gt; # ... with 74 more rows data(stop_words) text_word_count &lt;- text_df %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) %&gt;% # get rid of uninteresting words count(word, sort = TRUE) # count of each word left #&gt; Joining, by = &quot;word&quot; 5.2 unnest_tokens() on larger text Section 1.3 in online book. Then watch this video as you work through this section. Let’s look at a larger text, say all of Jane Austen’s novels. orig_books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() %&gt;% select(chapter, linenumber, everything()) orig_books #&gt; # A tibble: 73,422 x 4 #&gt; chapter linenumber text book #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; #&gt; 1 0 1 &quot;SENSE AND SENSIBILITY&quot; Sense &amp; Sensibility #&gt; 2 0 2 &quot;&quot; Sense &amp; Sensibility #&gt; 3 0 3 &quot;by Jane Austen&quot; Sense &amp; Sensibility #&gt; 4 0 4 &quot;&quot; Sense &amp; Sensibility #&gt; 5 0 5 &quot;(1811)&quot; Sense &amp; Sensibility #&gt; 6 0 6 &quot;&quot; Sense &amp; Sensibility #&gt; # ... with 73,416 more rows # make data tidy tidy_books &lt;- orig_books %&gt;% unnest_tokens(word, text) %&gt;% # use str_extract because some gutenberg texts have other symbols around # the words as part of the encoding mutate(word = str_extract(word, &quot;[a-z&#39;]+&quot;)) %&gt;% anti_join(stop_words) #&gt; Joining, by = &quot;word&quot; tidy_books %&gt;% count(word, sort = TRUE) #&gt; # A tibble: 13,464 x 2 #&gt; word n #&gt; * &lt;chr&gt; &lt;int&gt; #&gt; 1 miss 1860 #&gt; 2 time 1339 #&gt; 3 fanny 862 #&gt; 4 dear 822 #&gt; 5 lady 819 #&gt; 6 sir 807 #&gt; # ... with 13,458 more rows # visualize tidy_books %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 400) %&gt;% mutate(word = reorder(word,n)) %&gt;% ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() 5.3 Compare word frequencies between authors This section corresponds to section 1.4 in the online book. Watch this video as you work through this section. We now compare frequencies across different authors. We will look at H.G. Wells (The Island of Doctor Moreau, The War of the Worlds, The Time Machine, and The Invisible Man) and the Bronte Sisters (Jane Eyre, Wuthering Heights, Agnes Grey, The Tenant of Wildfell Hall and Villette) since they are from a similar time-frame as Jane Austen. First, take a few minutes to explore the gutenberg website. We will search by author and then find the book numbers we want to download. hgwells &lt;- gutenberg_download(c(35, 36, 159, 5230)) #&gt; Determining mirror for Project Gutenberg from http://www.gutenberg.org/robot/harvest #&gt; Using mirror http://aleph.gutenberg.org #&gt; Warning: `do()` is deprecated as of dplyr 1.0.0. #&gt; Use condense() or summarise() #&gt; This warning is displayed once every 8 hours. #&gt; Call `lifecycle::last_warnings()` to see where this warning was generated. bronte &lt;- gutenberg_download(c(767, 768, 969, 1260, 9182)) tidy_hgwells &lt;- hgwells %&gt;% unnest_tokens(word, text) %&gt;% mutate(word = str_extract(word, &quot;[a-z&#39;]+&quot;)) %&gt;% anti_join(stop_words) #&gt; Joining, by = &quot;word&quot; tidy_bronte &lt;- bronte %&gt;% unnest_tokens(word, text) %&gt;% mutate(word = str_extract(word, &quot;[a-z&#39;]+&quot;)) %&gt;% anti_join(stop_words) #&gt; Joining, by = &quot;word&quot; tidy_hgwells %&gt;% count(word, sort = TRUE) #&gt; # A tibble: 11,648 x 2 #&gt; word n #&gt; * &lt;chr&gt; &lt;int&gt; #&gt; 1 time 454 #&gt; 2 people 302 #&gt; 3 door 260 #&gt; 4 heard 249 #&gt; 5 black 232 #&gt; 6 stood 229 #&gt; # ... with 11,642 more rows tidy_bronte %&gt;% count(word, sort = TRUE) #&gt; # A tibble: 22,516 x 2 #&gt; word n #&gt; * &lt;chr&gt; &lt;int&gt; #&gt; 1 time 1065 #&gt; 2 miss 856 #&gt; 3 day 828 #&gt; 4 hand 768 #&gt; 5 eyes 713 #&gt; 6 night 647 #&gt; # ... with 22,510 more rows Put all three authors together in one tibble with a new column showing author. frequency_by_word_across_authors &lt;- bind_rows(mutate(tidy_bronte, author = &quot;Bronte&quot;), mutate(tidy_hgwells, author = &quot;Wells&quot;), mutate(tidy_books, author = &quot;Austen&quot;)) %&gt;% mutate(word = str_extract(word, &quot;[a-z&#39;]+&quot;)) %&gt;% count(author, word) %&gt;% group_by(author) %&gt;% mutate(proportion = n / sum(n)) %&gt;% select(-n) %&gt;% spread(author, proportion) frequency_by_word_across_authors #&gt; # A tibble: 28,678 x 4 #&gt; word Austen Bronte Wells #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 a&#39;most NA 0.0000160 NA #&gt; 2 a&#39;n&#39;t 0.00000462 NA NA #&gt; 3 aback NA 0.00000400 0.0000150 #&gt; 4 abaht NA 0.00000400 NA #&gt; 5 abandon NA 0.0000320 0.0000150 #&gt; 6 abandoned 0.00000462 0.0000920 0.000180 #&gt; # ... with 28,672 more rows frequency &lt;- frequency_by_word_across_authors %&gt;% gather(author, proportion, `Bronte`:`Wells`) frequency #&gt; # A tibble: 57,356 x 4 #&gt; word Austen author proportion #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 a&#39;most NA Bronte 0.0000160 #&gt; 2 a&#39;n&#39;t 0.00000462 Bronte NA #&gt; 3 aback NA Bronte 0.00000400 #&gt; 4 abaht NA Bronte 0.00000400 #&gt; 5 abandon NA Bronte 0.0000320 #&gt; 6 abandoned 0.00000462 Bronte 0.0000920 #&gt; # ... with 57,350 more rows 5.4 Compare word frequency by author to Austen Book sections 1.5 - 1.6 and this video Now let’s graph the frequency comparison of each other author to Jane Austen. frequency %&gt;% ggplot(aes(x = proportion, y = `Austen`, color = abs(`Austen` - proportion))) + geom_abline(color = &quot;gray40&quot;, lty = 2) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) + geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) + scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) + scale_color_gradient(limits = c(0, 0.001), low = &quot;darkslategray4&quot;, high = &quot;gray75&quot;) + facet_wrap(~author, ncol = 2) + theme(legend.position=&quot;none&quot;) + labs(y = &quot;Jane Austen&quot;, x = NULL) #&gt; Warning: Removed 41043 rows containing missing values (geom_point). #&gt; Warning: Removed 41045 rows containing missing values (geom_text). We can tell that Austen and Bronte are more similar (grouped closer to the line) than Austen and Wells. Let’s use a correlation test to quantify the amounts. df_Bronte &lt;- frequency[frequency$author == &quot;Bronte&quot;,] df_Bronte #&gt; # A tibble: 28,678 x 4 #&gt; word Austen author proportion #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 a&#39;most NA Bronte 0.0000160 #&gt; 2 a&#39;n&#39;t 0.00000462 Bronte NA #&gt; 3 aback NA Bronte 0.00000400 #&gt; 4 abaht NA Bronte 0.00000400 #&gt; 5 abandon NA Bronte 0.0000320 #&gt; 6 abandoned 0.00000462 Bronte 0.0000920 #&gt; # ... with 28,672 more rows cor.test(data = df_Bronte, ~ proportion + `Austen`) #&gt; #&gt; Pearson&#39;s product-moment correlation #&gt; #&gt; data: proportion and Austen #&gt; t = 119, df = 10299, p-value &lt;2e-16 #&gt; alternative hypothesis: true correlation is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.753 0.769 #&gt; sample estimates: #&gt; cor #&gt; 0.761 df_Wells &lt;- frequency[frequency$author == &quot;Wells&quot;,] df_Wells #&gt; # A tibble: 28,678 x 4 #&gt; word Austen author proportion #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 a&#39;most NA Wells NA #&gt; 2 a&#39;n&#39;t 0.00000462 Wells NA #&gt; 3 aback NA Wells 0.0000150 #&gt; 4 abaht NA Wells NA #&gt; 5 abandon NA Wells 0.0000150 #&gt; 6 abandoned 0.00000462 Wells 0.000180 #&gt; # ... with 28,672 more rows cor.test(data = df_Wells, ~ proportion + `Austen`) #&gt; #&gt; Pearson&#39;s product-moment correlation #&gt; #&gt; data: proportion and Austen #&gt; t = 36, df = 6010, p-value &lt;2e-16 #&gt; alternative hypothesis: true correlation is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.403 0.445 #&gt; sample estimates: #&gt; cor #&gt; 0.424 Exercise 1: Repeat the above analysis using all the H.G. Wells and Bronte works that are available on gutenberg.org You will need to use gutenberg_works(author == “Wells, H. G. (Herbert George)”) to get started. We figured this out by looking at any H.G. Wells book on the gutenberg.org website and then looking at the Bibiliography Record to see how the author is listed there. Similarly, find the Bronte works. Exercise 2: Pick three other authors from Gutenberg.org and download their works. Compare the authors. Which two are more alike? Some suggestions if you can’t think of any: Mark Twain, Leo Tolstoy, Charles Dickens. #&gt; Warning: `filter_()` is deprecated as of dplyr 0.7.0. #&gt; Please use `filter()` instead. #&gt; See vignette(&#39;programming&#39;) for more help #&gt; This warning is displayed once every 8 hours. #&gt; Call `lifecycle::last_warnings()` to see where this warning was generated. #&gt; Warning: `distinct_()` is deprecated as of dplyr 0.7.0. #&gt; Please use `distinct()` instead. #&gt; See vignette(&#39;programming&#39;) for more help #&gt; This warning is displayed once every 8 hours. #&gt; Call `lifecycle::last_warnings()` to see where this warning was generated. #&gt; Joining, by = &quot;word&quot; #&gt; Joining, by = &quot;word&quot; #&gt; # A tibble: 61,913 x 4 #&gt; word Austen Bronte Wells #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 &#39; NA NA 0.00000126 #&gt; 2 a&#39;ch NA NA 0.000000628 #&gt; 3 a&#39;chitect NA NA 0.00000126 #&gt; 4 a&#39;eplane NA NA 0.000000628 #&gt; 5 a&#39;hm NA NA 0.000000628 #&gt; 6 a&#39;ll NA NA 0.00000188 #&gt; # ... with 61,907 more rows #&gt; # A tibble: 123,826 x 4 #&gt; word Austen author proportion #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 &#39; NA Bronte NA #&gt; 2 a&#39;ch NA Bronte NA #&gt; 3 a&#39;chitect NA Bronte NA #&gt; 4 a&#39;eplane NA Bronte NA #&gt; 5 a&#39;hm NA Bronte NA #&gt; 6 a&#39;ll NA Bronte NA #&gt; # ... with 123,820 more rows #&gt; Warning: Removed 101057 rows containing missing values (geom_point). #&gt; Warning: Removed 101059 rows containing missing values (geom_text). #&gt; # A tibble: 61,913 x 4 #&gt; word Austen author proportion #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 &#39; NA Bronte NA #&gt; 2 a&#39;ch NA Bronte NA #&gt; 3 a&#39;chitect NA Bronte NA #&gt; 4 a&#39;eplane NA Bronte NA #&gt; 5 a&#39;hm NA Bronte NA #&gt; 6 a&#39;ll NA Bronte NA #&gt; # ... with 61,907 more rows #&gt; #&gt; Pearson&#39;s product-moment correlation #&gt; #&gt; data: proportion and Austen #&gt; t = 119, df = 10914, p-value &lt;2e-16 #&gt; alternative hypothesis: true correlation is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.742 0.758 #&gt; sample estimates: #&gt; cor #&gt; 0.75 #&gt; # A tibble: 61,913 x 4 #&gt; word Austen author proportion #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 &#39; NA Wells 0.00000126 #&gt; 2 a&#39;ch NA Wells 0.000000628 #&gt; 3 a&#39;chitect NA Wells 0.00000126 #&gt; 4 a&#39;eplane NA Wells 0.000000628 #&gt; 5 a&#39;hm NA Wells 0.000000628 #&gt; 6 a&#39;ll NA Wells 0.00000188 #&gt; # ... with 61,907 more rows #&gt; #&gt; Pearson&#39;s product-moment correlation #&gt; #&gt; data: proportion and Austen #&gt; t = 45, df = 11851, p-value &lt;2e-16 #&gt; alternative hypothesis: true correlation is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.365 0.396 #&gt; sample estimates: #&gt; cor #&gt; 0.381 "],
["tidy-text-mining-from-stat-413-continued.html", "6 Tidy text mining from STAT 413 continued 6.1 Sentiment Analysis 6.2 Plot a sentiment by chapter 6.3 Modifying what contributes to sentiment analysis 6.4 WordCloud plots", " 6 Tidy text mining from STAT 413 continued Learning Objectives: Analyze sentiment as it changes through a text Graph word clouds Resources: Text Mining with R by Julia Silge and David Robinson. 6.1 Sentiment Analysis Sections 2.1 - 2.3 in the book and this video Naive approach: sentiment of each word and add them up for a given amount of text. This approach does not take into account word qualifiers like not, never, always, etc. Generally, if we add up over many paragraphs, the positive and negative words will cancel each other out. So, we are usually better off adding either by sentence or by paragraph. There are several sentiment lexicons we can use: AFINN from Finn ?rup Nielsen, bing from Bing Liu and collaborators nrc from Saif Mohammad and Peter Turney sentiments #&gt; # A tibble: 6,786 x 2 #&gt; word sentiment #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2-faces negative #&gt; 2 abnormal negative #&gt; 3 abolish negative #&gt; 4 abominable negative #&gt; 5 abominably negative #&gt; 6 abominate negative #&gt; # ... with 6,780 more rows sentiments %&gt;% arrange(word) #&gt; # A tibble: 6,786 x 2 #&gt; word sentiment #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2-faces negative #&gt; 2 abnormal negative #&gt; 3 abolish negative #&gt; 4 abominable negative #&gt; 5 abominably negative #&gt; 6 abominate negative #&gt; # ... with 6,780 more rows get_sentiments(&quot;afinn&quot;) #&gt; # A tibble: 2,477 x 2 #&gt; word value #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 abandon -2 #&gt; 2 abandoned -2 #&gt; 3 abandons -2 #&gt; 4 abducted -2 #&gt; 5 abduction -2 #&gt; 6 abductions -2 #&gt; # ... with 2,471 more rows get_sentiments(&quot;bing&quot;) #&gt; # A tibble: 6,786 x 2 #&gt; word sentiment #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2-faces negative #&gt; 2 abnormal negative #&gt; 3 abolish negative #&gt; 4 abominable negative #&gt; 5 abominably negative #&gt; 6 abominate negative #&gt; # ... with 6,780 more rows get_sentiments(&quot;nrc&quot;) #&gt; # A tibble: 13,901 x 2 #&gt; word sentiment #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 abacus trust #&gt; 2 abandon fear #&gt; 3 abandon negative #&gt; 4 abandon sadness #&gt; 5 abandoned anger #&gt; 6 abandoned fear #&gt; # ... with 13,895 more rows Since the nrc lexicon gives us the emotion, we can look at words labelled as fear if we choose. tidy_books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() %&gt;% # use word so the inner_join will match with the nrc lexicon unnest_tokens(word, text) # select only the words from the nrc lexicon that are &quot;fear&quot; words nrcfear &lt;- get_sentiments(&quot;nrc&quot;) %&gt;% filter(sentiment == &quot;fear&quot;) tidy_books %&gt;% filter(book == &quot;Emma&quot;) %&gt;% inner_join(nrcfear) %&gt;% count(word, sort = TRUE) #&gt; # A tibble: 364 x 2 #&gt; word n #&gt; * &lt;chr&gt; &lt;int&gt; #&gt; 1 doubt 98 #&gt; 2 ill 72 #&gt; 3 afraid 65 #&gt; 4 marry 63 #&gt; 5 change 61 #&gt; 6 bad 60 #&gt; # ... with 358 more rows 6.2 Plot a sentiment by chapter Watch this video See if you can plot the fear by chapter. fear_chapter &lt;- tidy_books %&gt;% inner_join(nrcfear) %&gt;% group_by(book,chapter) %&gt;% count() fear_chapter %&gt;% ggplot(aes(chapter, n)) + geom_line() + facet_wrap(~book, scales = &quot;free_x&quot;) What other sentiments are there in nrc that we could look at? get_sentiments(&quot;nrc&quot;) %&gt;% group_by(sentiment) %&gt;% count() #&gt; # A tibble: 10 x 2 #&gt; # Groups: sentiment [10] #&gt; sentiment n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 anger 1247 #&gt; 2 anticipation 839 #&gt; 3 disgust 1058 #&gt; 4 fear 1476 #&gt; 5 joy 689 #&gt; 6 negative 3324 #&gt; # ... with 4 more rows Now, let’s use 80 line blocks and use bing to categorize each word as positive or negative. We will spread them to get the counts in separate columns and then add a column with the net = positive - negative janeaustensentiment &lt;- tidy_books %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(book, index = linenumber %/% 80, sentiment) %&gt;% spread(sentiment, n, fill = 0) %&gt;% mutate(sentiment = positive - negative) janeaustensentiment %&gt;% ggplot(aes(index, sentiment, fill = book)) + geom_col(show.legend = FALSE) + facet_wrap(~book, ncol = 2, scales = &quot;free_x&quot;) 6.3 Modifying what contributes to sentiment analysis Section 2.4 in online book and thisvideo We should probably look at which words contribute to the positive and negative sentiment and be sure we want to include them. bing_word_counts &lt;- tidy_books %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% ungroup() bing_word_counts #&gt; # A tibble: 2,585 x 3 #&gt; word sentiment n #&gt; * &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 miss negative 1855 #&gt; 2 well positive 1523 #&gt; 3 good positive 1380 #&gt; 4 great positive 981 #&gt; 5 like positive 725 #&gt; 6 better positive 639 #&gt; # ... with 2,579 more rows # visualize it bing_word_counts %&gt;% group_by(sentiment) %&gt;% top_n(10) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + labs(y = &quot;Contribution to sentiment&quot;, x = NULL) + coord_flip() Not what we want for Jane Austen novels!! Miss is probably not a negative word, but rather refers to a young girl. Two approaches to fix this: take the word miss out of the data before doing the analysis or change the sentiment lexicon to no longer have “miss” as a negative First we will remove the word miss by adding it to the stop words. custom_stop_words &lt;- bind_rows(data_frame( word = c(&quot;miss&quot;), lexicon = c(&quot;custom&quot;)), stop_words) custom_stop_words #&gt; # A tibble: 1,150 x 2 #&gt; word lexicon #&gt; * &lt;chr&gt; &lt;chr&gt; #&gt; 1 miss custom #&gt; 2 a SMART #&gt; 3 a&#39;s SMART #&gt; 4 able SMART #&gt; 5 about SMART #&gt; 6 above SMART #&gt; # ... with 1,144 more rows # Now, let&#39;s redo with the new stop words. tidy_books_no_miss &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() %&gt;% # use word so the inner_join will match with the nrc lexicon unnest_tokens(word, text) %&gt;% anti_join(custom_stop_words) bing_word_counts &lt;- tidy_books_no_miss %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% ungroup() bing_word_counts #&gt; # A tibble: 2,554 x 3 #&gt; word sentiment n #&gt; * &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 happy positive 534 #&gt; 2 love positive 495 #&gt; 3 pleasure positive 462 #&gt; 4 poor negative 424 #&gt; 5 happiness positive 369 #&gt; 6 comfort positive 292 #&gt; # ... with 2,548 more rows bing_word_counts %&gt;% group_by(sentiment) %&gt;% top_n(10) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + labs(y = &quot;Contribution to sentiment&quot;, x = NULL) + coord_flip() A different approach would be to leave it in the analysis, but remove the word “miss” from the bing sentiment lexicon. bing_no_miss &lt;- get_sentiments(&quot;bing&quot;) %&gt;% filter(word != &quot;miss&quot;) bing_word_counts &lt;- tidy_books %&gt;% inner_join(bing_no_miss) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% ungroup() bing_word_counts #&gt; # A tibble: 2,584 x 3 #&gt; word sentiment n #&gt; * &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 well positive 1523 #&gt; 2 good positive 1380 #&gt; 3 great positive 981 #&gt; 4 like positive 725 #&gt; 5 better positive 639 #&gt; 6 enough positive 613 #&gt; # ... with 2,578 more rows # visualize it bing_word_counts %&gt;% group_by(sentiment) %&gt;% top_n(10) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + labs(y = &quot;Contribution to sentiment&quot;, x = NULL) + coord_flip() Exercise 3: Let’s look at how the sentiment changes across the length of a book by looking at 80 lines at a time. Compare how sentiment changes in Victor Hugo’s Les Miserables and Charles Dickens’ A Tale of Two Cities. Look at negative vs positive sentiment. Then pick a sentiment like joy or anger or fear or … and see how that sentiment compares. 6.4 WordCloud plots Sections 2.5 - 2.7 in book and this video We can do wordcloud plots where the frequency of the word in the text determines the size of the word in the wordcloud. We can also color the words based on the sentiment. library(wordcloud) tidy_books %&gt;% anti_join(stop_words) %&gt;% count(word) %&gt;% with(wordcloud(word, n, max.words = 100)) library(reshape2) tidy_books %&gt;% inner_join(bing_no_miss) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;red&quot;, &quot;blue&quot;), max.words = 100) "],
["music-analysis-intro.html", "7 Introduction to Music Analysis", " 7 Introduction to Music Analysis What this module is about. "],
["music-1st-chapter-header.html", "8 Music 1st chapter header", " 8 Music 1st chapter header First chapter in music analysis after the intro. Here is how you link to a YouTube video. "],
["ethics-intro.html", "9 Introduction to Ethics and Machine Learning", " 9 Introduction to Ethics and Machine Learning What this module is about. "],
["machine-learning-intro.html", "10 Machine Learning Intro", " 10 Machine Learning Intro First chapter in machine learning and ethics. Here is how you link to a YouTube video. "]
]
